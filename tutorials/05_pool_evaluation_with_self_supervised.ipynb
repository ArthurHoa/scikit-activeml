{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33f39229508719ea",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## **Tutorial Overview: Comparing Active Learning Strategies with scikit-activeml**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4583e83ede8da9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "This tutorial aims to demonstrate a practical comparison study using the ```scikit-activeml``` library [1]. The workflow involves utilizing a self-supervised learning model, specifically ```DINOv2``` from [2 - 4], to generate embeddings for the Flowers-102 datasets. Subsequently, various active learning strategies will be employed to intelligently select samples for labeling.\n",
    "\n",
    "**Key Steps:**\n",
    "1. **Self-Supervised Learning Model:** Utilize the DINOv2 model to create embedding datasets for CIFAR-10, CIFAR-100, and Flowers-102 datasets.\n",
    "\n",
    "2. **Active Learning Strategies:** Employ different active learning strategies provided by the scikit-activeml library, including:\n",
    "    - Random Sampling\n",
    "    - Uncertainty Sampling\n",
    "    - Discriminative Active Learning (DiscriminativeAL)\n",
    "    - CoreSet\n",
    "    - TypiClust\n",
    "    - Badge\n",
    "\n",
    "3. **Labeling Selection:** Use each active learning strategy to select specific samples for labeling, exploring diverse approaches to guide the learning process.\n",
    "\n",
    "4. **Plotting the results:** By the end of this notebook, we'll have a comparision on the accuray of the aforementioned active learning strategies.\n",
    "\n",
    "**References:**\n",
    "\n",
    "[1] https://scikit-activeml.github.io/scikit-activeml-docs/index.html\n",
    "\n",
    "[2] M. Oquab et al., ‘DINOv2: Learning Robust Visual Features without Supervision’. arXiv, Apr. 14, 2023. Accessed: Jan. 13, 2024. [Online]. Available: http://arxiv.org/abs/2304.07193\n",
    "\n",
    "[3] https://dinov2.metademolab.com\n",
    "\n",
    "[4] https://github.com/facebookresearch/dinov2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4beb9cfd-4f18-4c44-bc55-542cba4a6479",
   "metadata": {},
   "source": [
    "**Import Packages**\n",
    "\n",
    "First, let's import the package we will beed for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b260866-c6ad-4f9e-84b1-ca621c04caea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U matplotlib\n",
    "#!pip install -U scikit-learn\n",
    "#!pip install iteration_utiliti\n",
    "#!pip3 install torch torchvision torchaudio\n",
    "#!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c7783b6-92f9-4402-a38d-69ee99b7a722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/mnt/stud/home/jcheng/scikit-activeml/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a4b6dbb9143a5eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T19:28:22.413502Z",
     "start_time": "2024-01-17T19:28:18.091576Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mlp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from skactiveml.classifier import SklearnClassifier\n",
    "from skactiveml.pool import UncertaintySampling, RandomSampling, DiscriminativeAL, CoreSet, TypiClust, Badge\n",
    "from skactiveml.utils import call_func, MISSING_LABEL\n",
    "\n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "mlp.rcParams[\"figure.facecolor\"] = \"white\"\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac32ea901e08c01",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Prepair your Data with DINOv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4e3bf805669b1c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "In this step, we focus on preparing the datasets using the powerful self-supervised learning model DINOv2. DINOv2, short for \"self-distillation with no labels\", is a state-of-the-art model that excels at learning meaningful representations from unlabelled data.\n",
    "\n",
    "If you've already completed these steps, you can skip ahead to loading your data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0819f4-b45f-4285-aa1d-84469764b6a5",
   "metadata": {},
   "source": [
    "**Step 1: Transformation**\n",
    "\n",
    "Apply necessary transformations to the datasets, including resizing images to a standardised format. This ensures consistency of input dimensions to the DINOv2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b27a5c395f5897e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T13:23:20.397158Z",
     "start_time": "2024-01-17T13:23:20.392522Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "transforms = transforms.Compose(\n",
    "        [transforms.Resize(256),\n",
    "         transforms.CenterCrop(224),\n",
    "         transforms.ToTensor(),\n",
    "         transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))]\n",
    "    )\n",
    "\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0493d78a-c3d5-42b3-b5b6-b83191d4bbe1",
   "metadata": {},
   "source": [
    "**Step 2: Load pretrained Model**\n",
    "To calculate embeddings, we'll use DINOv2. Below we load the second smallest DINOv2 model to generate embedding datasets for the Flowers-102 datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "387db871-7e26-46d0-9722-45c71ed40014",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /mnt/stud/home/jcheng/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DinoVisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x NestedTensorBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): MemEffAttention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Identity()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dinov2_vitb14 = torch.hub.load(\"facebookresearch/dinov2\", \"dinov2_vitb14\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "dinov2_vitb14.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7668a073b61085ea",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**Step 3: Load Datasets and Generate Embeddings**\n",
    "\n",
    "Firstly, we begin by loading the CIFAR-10, CIFAR-100, and Flowers-102 datasets.\n",
    "\n",
    "After that, we employ the pre-trained DINOv2 model to generate embeddings for each image in the datasets and save them in a npy file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c0165be-d3ce-4eb2-9309-45ba6c08c8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_dataset(dataset_name, root_dir, num_classes, is_train):\n",
    "    # Load the dataset\n",
    "    if is_train:\n",
    "        split = 'train'\n",
    "    else:\n",
    "        split = 'val'\n",
    "    \n",
    "    dataset = datasets.__dict__[dataset_name](root=root_dir, split=split, download=True, transform=transforms)\n",
    "        \n",
    "\n",
    "    # Create a DataLoader\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=is_train, num_workers=2)\n",
    "\n",
    "    embedding_list = []\n",
    "    label_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"{dataset_name.capitalize()} {split}\"):\n",
    "            image, label = data\n",
    "            embeddings = dinov2_vitb14(image.to(device)).cpu()\n",
    "            embedding_list.append(embeddings)\n",
    "            label_list.append(label)\n",
    "\n",
    "        # Concatenate embeddings and labels\n",
    "        X = torch.cat(embedding_list, dim=0).numpy()\n",
    "        y_true = torch.cat(label_list, dim=0).numpy()\n",
    "\n",
    "    return X, y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ac7d46-ab15-4aaf-ad72-2f600c227cd9",
   "metadata": {},
   "source": [
    "Applying on Flowers102 Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4abc295a-f41d-42aa-9b02-3a9fffc28b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flowers102 train: 100%|██████████| 255/255 [00:43<00:00,  5.82it/s]\n",
      "Flowers102 val: 100%|██████████| 255/255 [00:43<00:00,  5.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# Flowers-102\n",
    "flowers102_X_train, flowers102_y_train_true = load_and_process_dataset(\"Flowers102\", \"./data\", 102, True)\n",
    "flowers102_X_test, flowers102_y_test_true = load_and_process_dataset(\"Flowers102\", \"./data\", 102, False)\n",
    "\n",
    "np.save('./embedding_data/flowers102_dinov2B_X_train.npy', flowers102_X_train)\n",
    "np.save('./embedding_data/flowers102_dinov2B_y_train.npy', flowers102_y_train_true)\n",
    "np.save('./embedding_data/flowers102_dinov2B_X_test.npy', flowers102_X_test)\n",
    "np.save('./embedding_data/flowers102_dinov2B_y_test.npy', flowers102_y_test_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0757f8-9261-4510-90cd-8a2a8707add7",
   "metadata": {},
   "source": [
    "## Load your preprocessed Dataset\n",
    "\n",
    "If you have previously processed your data with DINOv2, please use the following code to load your data. And we also define the number of classes in the Flowers102 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e29fc457-2fb1-4824-8cd5-b637e44795a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flowers-102\n",
    "X_train = np.load('./embedding_data/flowers102_dinov2B_X_train.npy')\n",
    "y_train_true = np.load('./embedding_data/flowers102_dinov2B_y_train.npy')\n",
    "X_test = np.load('./embedding_data/flowers102_dinov2B_X_test.npy')\n",
    "y_test_true = np.load('./embedding_data/flowers102_dinov2B_y_test.npy')\n",
    "\n",
    "dataset_classes = 102"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4565232ae31432",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Random Seed Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ce38a2-0305-44eb-abf5-cb69c41c3f98",
   "metadata": {},
   "source": [
    "To ensure experiment reproducibility, it's important to set random states for all components that might use them. For simplicity, we set a single fixed random state and use helper functions to generate new seeds and random states. It's important to note that the ```master_random_state``` should only be used to create new random states or random seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11e6ce648a9ab110",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T19:28:35.438877Z",
     "start_time": "2024-01-17T19:28:35.420949Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "master_random_state = np.random.RandomState(0)\n",
    "\n",
    "def gen_seed(random_state:np.random.RandomState):\n",
    "    return random_state.randint(0, 2**31)\n",
    "\n",
    "def gen_random_state(random_state:np.random.RandomState):\n",
    "    return np.random.RandomState(gen_seed(random_state))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cc0c5019b852b5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Classification Models and Query Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a3743e-cb9e-49a9-a239-30c9f54839fa",
   "metadata": {},
   "source": [
    "The embeddings we have computed can be used as an input in a classification model. For this guide, we will be using LogisticRegression from ```sklearn``` [5]. Moreover, we handle the creation of query strategies using factory functions to simplify the seperation.\n",
    "\n",
    "[5] ‘sklearn.linear_model.LogisticRegression’, scikit-learn. Accessed: Jan. 24, 2024. [Online]. Available: https://scikit-learn/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d356c3752b0a58a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T16:23:51.265616Z",
     "start_time": "2024-01-17T16:23:51.249552Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "clf = SklearnClassifier(LogisticRegression(), classes=np.arange(dataset_classes), random_state=gen_seed(master_random_state))\n",
    "\n",
    "def create_query_strategy(name, random_state):\n",
    "    return query_strategy_factory_functions[name](random_state)\n",
    "\n",
    "query_strategy_factory_functions = {\n",
    "    'RandomSampling': lambda random_state: RandomSampling(random_state=gen_seed(random_state)),\n",
    "    'UncertaintySampling': lambda random_state: UncertaintySampling(random_state=gen_seed(random_state)),\n",
    "    'DiscriminativeAL': lambda random_state: DiscriminativeAL(random_state=gen_seed(random_state)),\n",
    "    'CoreSet': lambda random_state: CoreSet(random_state=gen_seed(random_state)),\n",
    "    'TypiClust': lambda random_state: TypiClust(random_state=gen_seed(random_state)),\n",
    "    'Badge': lambda random_state: Badge(random_state=gen_seed(random_state))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9023c37048ec8f54",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Experiment Parameters\n",
    "\n",
    "For this experiment, we need to define how the strategies should be compared against one another. Here the number of repetitions (```n_reps```), the numer of queries (```n_cycles```), the size of each query (```query_batch_size```) need to be difined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e103670190e73a3f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n_reps = 1\n",
    "n_cycles = 30\n",
    "query_batch_size = 8\n",
    "query_strategy_names = query_strategy_factory_functions.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b8fff1b12211ed",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Experiment Loop\n",
    "\n",
    "The actual experiment loops all over the query strategies. The average accuracy over the test set is recorded for each cycle and stored in the ```results``` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718629bb-1c4f-4707-9eb1-9d1df901cd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repeat 1 with RandomSampling: 100%|██████████| 30/30 [00:26<00:00,  1.14it/s]\n",
      "Repeat 1 with UncertaintySampling: 100%|██████████| 30/30 [01:02<00:00,  2.09s/it]\n",
      "Repeat 1 with DiscriminativeAL: 100%|██████████| 30/30 [00:56<00:00,  1.88s/it]\n",
      "Repeat 1 with CoreSet: 100%|██████████| 30/30 [00:35<00:00,  1.17s/it]\n",
      "Repeat 1 with TypiClust:  97%|█████████▋| 29/30 [02:30<00:08,  8.30s/it]"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for qs_name in query_strategy_names:\n",
    "    accuracies = np.full((n_reps, n_cycles), np.nan)\n",
    "    for i_rep in range(n_reps):\n",
    "        y_train = np.full(shape=y_train_true.shape, fill_value=MISSING_LABEL)\n",
    "    \n",
    "        qs = create_query_strategy(qs_name, random_state=gen_random_state(master_random_state))\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        for c in tqdm(range(n_cycles), desc=f'Repeat {i_rep + 1} with {qs_name}'):\n",
    "            query_idx = call_func(qs.query, X=X_train, y=y_train, batch_size=query_batch_size, clf=clf, discriminator=clf)\n",
    "            y_train[query_idx] = y_train_true[query_idx]\n",
    "            clf.fit(X_train, y_train)\n",
    "            score = clf.score(X_test, y_test_true)\n",
    "            accuracies[i_rep, c] = score\n",
    "    \n",
    "    results[qs_name] = accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998e52cb1a2107b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Resulting Plotting\n",
    "\n",
    "We use learning curves to compare strategies. We visualise the average accuracy, averaged over all repetitions and folds, relative to the number of queries. In addition, the legend provides insight into the area under the learning curve, which indicates the average accuracy over all cycles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed6ebe52431f489",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for qs_name in query_strategy_names:\n",
    "    key = qs_name\n",
    "    result = results[key]\n",
    "    reshaped_result = result.reshape((-1, n_cycles))\n",
    "    errorbar_mean = np.mean(reshaped_result, axis=0)\n",
    "    plt.errorbar(np.arange(n_cycles), errorbar_mean, label=f\"({np.mean(errorbar_mean):.4f}) {qs_name}\", alpha=0.5)\n",
    "plt.title(\"LogisticRegression\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('cycle')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4509ff6a-d016-4db7-8990-778b55472678",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
